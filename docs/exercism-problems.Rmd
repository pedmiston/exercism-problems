---
title: "Analysis of Exercism.io problems"
output:
  html_document:
    theme: flatly
---
```{r config, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```
```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal())

library(exercism)
data("problem_specifications")
data("exercises")

exercises <- filter(exercises, difficulty != 0)
```

# Number of test cases

```{r n-test-cases-dotplot, fig.height=2}
ggplot(problem_specifications) +
  aes(n_test_cases) +
  geom_dotplot(binwidth = 1, stackdir = "center", fill = "gray") +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "number of test cases", caption = "Distribution of problems sorted by number of test cases.")
```

```{r n-test-cases-dotplot-labeled, fig.height=20, fig.width=40}
problem_specifications_dotplot <- problem_specifications %>%
  group_by(n_test_cases) %>%
  mutate(height = 1:n(),
         height_c = height - mean(height)) %>%
  ungroup()

ggplot(problem_specifications_dotplot) +
  aes(factor(n_test_cases), height_c) +
  geom_text(aes(label = exercise), angle = 45, hjust = 0.5, check_overlap = TRUE, size = 10) +
  coord_cartesian(xlim = c(-0.1, 28), ylim = c(-11, 11), expand = FALSE) +
  labs(x = "number of test cases", caption = "Distribution of labeled problems for reference. Note the ordinal x-axis.") +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(text = element_text(size = 50))
```

# Difficulty of the same problems in different languages

Most Exercism.io problems have been assigned a difficulty score ranging from
1-10. For example, [python's "hello-world" problem is assigned a difficulty of
1](https://github.com/exercism/python/blob/master/config.json#L15). I don't know
how these difficulty scores are assigned. It's likely they are self-assigned by
the developers working on Exercism.io, and used in the ordering of problems for
particular learning tracks. What's interesting about these scores is that the
same problems are assigned different difficulty scores in different languages.
Note the variability within each problem in the plot below.

```{r difficulty, fig.height=14}
set.seed(254)
ggplot(exercises) +
  aes(fct_reorder(exercise, difficulty, .fun = mean), difficulty) +
  stat_summary(fun.y = mean,
               fun.ymin = function(x) mean(x) - sd(x), 
               fun.ymax = function(x) mean(x) + sd(x), 
               geom = "linerange") +
  geom_point(aes(color = language), position = position_jitter(width=0, height=0.2), shape = 1, alpha = 0.6) +
  coord_flip() +
  scale_y_continuous(breaks = 1:10, position = "right") +
  labs(x = "", caption = "Problems sorted by average difficulty across languages. Lineranges show mean difficulties Â±1 standard deviation.") +
  theme(legend.position = "none")
```

# Average difficulty across languages

Are all Exercism.io problems easier in some languages than others? To test this,
we can estimate the average difficulty across all problems for each language while
controlling for problem using a hierarchical linear model.

```{r ranking, echo=1:2, fig.height=8}
# Fit lmer mod with one param per language
lang_difficulty_mod <- lme4::lmer(difficulty ~ -1 + language + (1|exercise),
                                  data = exercises)

lang_difficulty_preds <- broom::tidy(lang_difficulty_mod, effects = "fixed") %>%
  rename(language = term) %>%
  mutate(language = str_replace(language, "^language", ""))

ggplot(lang_difficulty_preds) +
  aes(fct_reorder(language, estimate, .desc = TRUE), estimate) +
  geom_linerange(aes(ymin = estimate - std.error, ymax = estimate + std.error)) +
  coord_flip(ylim = c(0, 5.9), expand = FALSE, clip = "off") +
  labs(x = "", y = "average difficulty")
```

## Looking only at the core exercises implemented in the most popular languages

```{r ranking-core, echo=TRUE, fig.height=4}
# Select problems with implementations in all languages
n_languages <- length(unique(exercises$language))
problems_in_all_languages <- count(exercises, exercise) %>%
  filter(n == n_languages)
# No problems in all languages!

# Select problems with implementations in the 10 most popular languages according to StackOverflow
exercism_languages <- unique(exercises$language)
data("stack_overflow_ranks", package = "programmingquestionnaire")
top_10_languages <- stack_overflow_ranks %>%
  filter(language_name %in% exercism_languages) %>%
  top_n(10) %>%
  .$language_name
exercises_in_all_top_10_languages <- exercises %>%
  group_by(exercise) %>%
  summarize(core = all(top_10_languages %in% language)) %>%
  filter(core) %>%
  .$exercise
core_exercises <- filter(exercises, language %in% top_10_languages, exercise %in% exercises_in_all_top_10_languages)

core_lang_difficulty_mod <- lme4::lmer(difficulty ~ -1 + language + (1|exercise),
                                       data = core_exercises)

core_lang_difficulty_preds <- broom::tidy(core_lang_difficulty_mod, effects = "fixed") %>%
  rename(language = term) %>%
  mutate(language = str_replace(language, "^language", ""))

ggplot(core_lang_difficulty_preds) +
  aes(fct_reorder(language, estimate, .desc = TRUE), estimate) +
  geom_linerange(aes(ymin = estimate - std.error, ymax = estimate + std.error)) +
  coord_flip(ylim = c(0, 5.9), expand = FALSE, clip = "off") +
  labs(x = "", y = "average difficulty")

mod_comparison <- left_join(core_lang_difficulty_preds[,c("language", "estimate")],
          lang_difficulty_preds[,c("language", "estimate")],
          by = "language", suffix = c("_core", "_overall"))
ggplot(mod_comparison) +
  aes(estimate_core, estimate_overall, label = language) +
  geom_text(check_overlap = TRUE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "estimated difficulty across core problems",
       y = "estimated difficulty across all problems") +
  coord_equal(xlim = c(1, 6), ylim = c(1, 6)) +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = 1:6)
```
