---
title: "Solving the same problems in different programming languages"
output:
  github_document:
    toc: yes
    toc_depth: 2
---
```{r config, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
read_chunk("R/exercism-problems.R")
```
```{r setup, include=FALSE}
```

Are the same problems easier to solve in some programming languages than others?
To answer this question, we analyzed the difficulty of the same programming problems
solved in different languages available on the website Exercism.io.

## What is Exercism.io?

[Exercism.io](https://exercism.io) is a website for users to learn new
programming languages and improve their programming ability by completing
practice problems referred to as "exercises". To complete each exercise, users
have to write a program that meets certain requirements as described in the
problem specification and evaluated by automated tests. On Exercism.io, the same
problems can be solved in different languages, making data about solutions to
Exercism.io problems valuable for understanding the potential differences between
languages.

## Self-assigned problem difficulty

```{r difficulty, include=FALSE}
```

Most Exercism.io problems have been assigned a difficulty score ranging from
1-10. For example, [python's "hello-world"
problem](https://github.com/exercism/python/blob/master/config.json#L15) is
assigned a difficulty of 1.

> I don't know how these difficulty scores are assigned. I will assume they are
  self-assigned by the developers working on Exercism.io, used in the ordering of
  problems for particular learning tracks, and are at least somewhat correlated
  with behavioral measures of difficulty.

What's interesting about these difficulty scores is that the same problems are
assigned different difficulty scores in different languages. Note the
variability within each problem (row) in the plot below.

```{r difficulty-per-problem, fig.height=14}
difficulty_plot
```

## Difficulty per language

**Are all problems easier in some languages than others?** We can estimate the
average difficulty for each language while controlling for overall problem
difficulty using a hierarchical linear model.

```{r difficulty-per-language, fig.height=8}
lang_difficulty_plot
```

Not every exercise is implemented in every language. **Core exercises** are
implemented in all 10 most popular languages according to StackOverflow. How do
these top languages compare in self-assigned difficulty on the
core exercises?

```{r core-difficulty}
core_difficulty_plot +
  labs(title = "Difficulty of core exercises",
       caption = "Difficulty of core exercises implemented in all top 10 most popular languages.")
```

## Number of test cases

```{r n-test-cases, include=FALSE}
```

Another factor to incorporate into our model is the number of **test cases**.
Test cases involve providing the solution program with input and comparing the
output to what was expected. The same test cases can be evaluated in all
languages. Each problem has a different number of test cases, and the
distribution of problem sizes is shown below.

> Problems may have more tests than test cases because they
  specify behavioral requirements (e.g., returning an object of a
  particular type) that do not correspond to input and output pairs.

```{r n-test-cases-dotplot, fig.height=3}
n_test_cases_plot
```

## Difficulty per test case

**What's the relationship between number of test cases and difficulty?**
Presumably problems with more test cases would be harder, and yet not every test
is the same level of difficulty.

```{r problem-difficulty-by-num-tests}
difficulty_per_test_case_plot
```

## Exercise topics

```{r topics, include=FALSE}
```

Each problem touches on one or more topic.

```{r top20-topics}
top20_topics_plot
```

## Difficulty by topic

Some topics are more difficult than others.

```{r topic-difficulty}
difficulty_by_topic_plot
```
