---
title: "Analysis of Exercism.io problems"
output:
  html_document:
    theme: flatly
---
```{r config, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
```
```{r setup, include=FALSE}
library(tidyverse)
theme_set(theme_minimal())

library(exercism)
data("problem_specifications")
data("exercises")

exercises <- filter(exercises, difficulty != 0)

# Select core exercises: Problems with implementations in the 10 most popular languages according to StackOverflow.
exercism_languages <- unique(exercises$language)
data("stack_overflow_ranks", package = "programmingquestionnaire")
top_10_languages <- stack_overflow_ranks %>%
  filter(language_name %in% exercism_languages) %>%
  top_n(10) %>%
  .$language_name
core_exercises <- exercises %>%
  filter(difficulty > 0) %>%
  group_by(exercise) %>%
  summarize(is_core = all(top_10_languages %in% language)) %>%
  filter(is_core) %>%
  .$exercise

problem_specifications <- mutate(problem_specifications, is_core = exercise %in% core_exercises)
```

# Number of test cases

The problems or "exercises" on Exercism.io are scored via automated tests. These
tests often involve providing the solution program with input and
comparing the output to what was expected. These **test cases** can
be scored in all languages. Each problem has a different number of test cases,
and the distribution of problem sizes is shown below. The existence of problems
with zero test cases could mean that the problem is still being developed. But
most problems have more tests than test cases, because many problems specify
certain behavioral requirements (e.g., returning an object of a particular type)
that must be manually translated into tests in each language.

**Core exercises** are implemented in all 10 most popular languages according to StackOverflow.

```{r n-test-cases-dotplot, fig.height=3}
scale_color_is_core <- scale_color_brewer("", labels = c("other", "core"), palette = "Set2")
scale_fill_is_core <- scale_fill_brewer("", labels = c("other", "core"), palette = "Set2")
ggplot(problem_specifications) +
  aes(n_test_cases, fill = is_core) +
  geom_dotplot(binwidth = 1, stackdir = "center") +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_fill_is_core +
  labs(x = "number of test cases", caption = "Distribution of problems sorted by number of test cases.\nSource: github.com/exercism/problem-specifications")
```

```{r n-test-cases-dotplot-labeled, fig.height=20, fig.width=40}
problem_specifications_dotplot <- problem_specifications %>%
  group_by(n_test_cases) %>%
  mutate(height = 1:n(),
         height_c = height - mean(height)) %>%
  ungroup()

ggplot(problem_specifications_dotplot) +
  aes(factor(n_test_cases), height_c) +
  geom_text(aes(label = exercise, color = is_core), angle = 45, hjust = 0.5, check_overlap = TRUE, size = 10) +
  coord_cartesian(xlim = c(-0.1, 28), ylim = c(-11, 11), expand = FALSE) +
  labs(x = "number of test cases", caption = "Distribution of labeled problems for reference. Note the ordinal x-axis.\nSource: github.com/exercism/problem-specifications") +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_color_is_core +
  theme(text = element_text(size = 50))
```

# Difficulty of the same problems in different languages

Most Exercism.io problems have been assigned a difficulty score ranging from
1-10. For example, [python's "hello-world" problem is assigned a difficulty of
1](https://github.com/exercism/python/blob/master/config.json#L15). I don't know
how these difficulty scores are assigned. It's likely they are self-assigned by
the developers working on Exercism.io, and used in the ordering of problems for
particular learning tracks. What's interesting about these scores is that the
same problems are assigned different difficulty scores in different languages.
Note the variability within each problem in the plot below.

```{r difficulty, fig.height=14}
set.seed(254)
ggplot(exercises) +
  aes(fct_reorder(exercise, difficulty, .fun = mean), difficulty) +
  stat_summary(fun.y = mean,
               fun.ymin = function(x) mean(x) - sd(x), 
               fun.ymax = function(x) mean(x) + sd(x), 
               geom = "linerange") +
  geom_point(aes(color = language), position = position_jitter(width=0, height=0.2), shape = 1, alpha = 0.6) +
  coord_flip() +
  scale_y_continuous(breaks = 1:10, position = "right") +
  labs(x = "", caption = "Problems sorted by average difficulty across languages. Lineranges show mean difficulties Â±1 standard deviation.") +
  theme(legend.position = "none")
```

# Average difficulty across languages

Are all Exercism.io problems easier in some languages than others? To test this,
we can estimate the average difficulty across all problems for each language while
controlling for problem using a hierarchical linear model.

```{r ranking, echo=1:2, fig.height=8}
# Fit lmer mod with one param per language
lang_difficulty_mod <- lme4::lmer(difficulty ~ -1 + language + (1|exercise),
                                  data = exercises)

get_lang_difficulty_preds <- function(mod) {
  broom::tidy(mod, effects = "fixed") %>%
    rename(language = term) %>%
    mutate(language = str_replace(language, "^language", ""))
}

lang_difficulty_preds <- get_lang_difficulty_preds(lang_difficulty_mod)
lang_difficulty_plot <- ggplot(lang_difficulty_preds) +
    aes(fct_reorder(language, estimate, .desc = TRUE), estimate) +
    geom_linerange(aes(ymin = estimate - std.error, ymax = estimate + std.error)) +
    coord_flip(ylim = c(0.7, 5.9), expand = FALSE, clip = "off") +
    labs(x = "", y = "average difficulty") +
    scale_y_continuous(breaks = 1:6)
lang_difficulty_plot
```

## Core exercises

How do the top 10 most popular languages compare in self-assigned difficulty on the core
exercises?

```{r ranking-core}
core_lang_difficulty_mod <- lme4::lmer(difficulty ~ -1 + language + (1|exercise),
                                  data = filter(exercises, language %in% top_10_languages, exercise %in% core_exercises))
core_lang_difficulty_preds <- get_lang_difficulty_preds(core_lang_difficulty_mod)
lang_difficulty_plot %+% core_lang_difficulty_preds
```

Are the core exercises really so easy in ruby and python compared to java and typescript? One
potential confound is that the core exercises might simply be easier overall, in which case
we might conclude that ruby and python are good for easy problems. One way to show how
difficulty on the core problems correlates with overall difficulty is to
compare the model estimates from the full model to the core-only model.

```{r mod-estimate-correlations}
mod_comparison <- left_join(core_lang_difficulty_preds[,c("language", "estimate")],
          lang_difficulty_preds[,c("language", "estimate")],
          by = "language", suffix = c("_core", "_overall"))
ggplot(mod_comparison) +
  aes(estimate_overall, estimate_core, label = language) +
  geom_text(check_overlap = TRUE) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "estimated difficulty across all exercises",
       y = "estimated difficulty across core exercises",
       caption = "Correlation between the model estimates from the full model and the core-only model.\nCore exercises are easier in all languages.") +
  coord_equal(xlim = c(1, 6), ylim = c(1, 6)) +
  scale_x_continuous(breaks = 1:6) +
  scale_y_continuous(breaks = 1:6)
```
